Examples of programs benefiting from linear types fall into three
categories.

* Enforcement of protocol
Linear types can be used to encode protocols, in a way very similar to
'session types'. Linearity checks ensure that the protocol is
respected (so one does not backtrack or drops out).

The following example is inspired by "Embedding Session Types in
Haskell", Sam Lindley and J. Garrett Morris.

#+BEGIN_SRC haskell
pr :: Double -> Effect -- "prints" a number

type N a = a ⊸ Effect

data Client
  = Mul Double Double (N (Double ⊗ Server)) -- Client sends two 'Double' and
                                            -- expects a double and a new server
                                            -- session.
  | Terminate
type Server = N Client


exampleClient :: N (N Client)
exampleClient server = server $ Mul 12 34 $ \(product,server') ->
  -- do something with the product
  pr product <> server' Terminate

exampleServer :: Server
exampleServer client = case client of
  Mul x y k -> k (x*y,exampleServer)
  Terminate -> return ()
#+END_SRC

* Correctness of optimized code

Writing programs in the polarized style shown above is very useful to write efficient programs.

In general, fusion in GHC relies on the rewrite rules and the inliner.

1. Rewrite rules transform code using general recursion into a
   representation with no recursion (eg. church encodings)
2. The inliner kicks in and 'fuses' composition of non-recursive
   functions
3. Unfused code may be reverted to the original representation.

The problem with this scheme is that it involves two phases of
heuristics (rules and inliner), and in practice programmers have
difficulties to predict the performance of any given program.

A partial remedy to this situation is to stop relying on rewrite rules,
and use directly non-recursive representations. For example the
following representation from Lippmeier et
al. [http://benl.ouroborus.net/papers/2016-polarized/dpdf-FHPC2016-preprint.pdf]:

#+BEGIN_SRC haskell
data Sources i m e = Sources -- 'i' is the array's index type, 'e' the type of elements and 'm' the effects
  { arity :: i
  , pull  :: i -> (e -> m ()) -> m () -> m () } -- 'pull' is an iterator to apply to every elements of the array (like 'traverse')

data Sinks i m e = Sinks
  { arity :: i
  , push  :: i -> e -> m ()
  , eject :: i -> m () }
#+END_SRC
Such representations are typically functionals, and thus do not
consume memory. One eventually gets code which is guaranteed to be
'fused'. For instance, in the following example from Lippmeier et al.,
neither the source nor the sink represent data in memory.

#+BEGIN_SRC haskell
copySetP :: [FilePath] -> [FilePath] -> IO ()
copySetP srcs dsts = do
  ss <- sourceFs srcs
  sk <- sinkFs   dsts
  drainP ss sk
#+END_SRC
One then faces two classes of new problems.

First, any non-linear (precisely non-affine) use of such a representation will /duplicate work/. For example:

#+BEGIN_SRC haskell
example srcs dsts = do
  ss <- expensiveComputation <$> sourceFs srcs
  sk <- sinkFs  dsts
  drainP ss sk
  drainP ss sk -- expensiveComputation is run a second time here.
#+END_SRC

If one is not careful, one may end up with a program which does not
use any intermediate memory, but duplicates a lot of intermediate
computations. Linear types solve the problem by preventing such
duplications. (Combinators may be still provided to duplicate
computation explicitly or store intermediate results explicitly.)

Second, such representations may contain effects. In this situation,
non-linear uses may produce an /incorrect program/. If one takes
the example of a non-recursive representation of files, one may have
two processes writing simultaneously in the same file (potentially
corrupting data), or one may forget to close the file.

Quoting Lippmeier et al.:

#+BEGIN_QUOTE
  In general an object of type Sources is an abstract producer of
  data, and it may not even be possible to rewind it to a previous
  state — suppose it was connected to a stream of sensor
  readings. Alas the Haskell type system does not check linearity so
  we rely on the programmer to enforce it manually.
#+END_QUOTE

Literature on this style of non-recursive representations includes additionally:

- Push and Pull arrays in Feldspar https://scholar.google.com/scholar?oi=bibs&hl=en&cites=5417443716167196803
- On the duality of streams https://jyp.github.io/pdf/Organ.pdf
- Composable Efficient Array Computations using Linear Types http://www.cse.chalmers.se/~josefs/publications/vectorcomp.pdf



* Controlling sharing (full laziness)

According to de Vries
(http://www.well-typed.com/blog/2016/09/sharing-conduit/):

#+BEGIN_QUOTE
[...] In the presence of the full laziness optimization we have no
control over when values are not shared. While it is possible in
theory to write code in such a way that the lazy data structures are
self-referential and hence keeping them in memory does not cause a
memory leak, in practice the resulting code is too brittle and writing
code like this is just too difficult.

[..]

Full laziness can be disabled using ~-fno-full-laziness~, but sadly this
throws out the baby with the bathwater. In many cases, full laziness
is a useful optimization.
#+END_QUOTE

Linearity offers a solution to the problem. Indeed, linearly-typed
values are used once only. Thus, linearity implies that no sharing is
intended by the programmer. In turn, the full laziness optimization
cannot apply to expressions in a linear context.

Consider now a simple example which exhibits the problem, also provided
by de Vries: 

#+BEGIN_SRC haskell
ni_mapM_ :: (a -> IO b) -> [a] -> IO ()
{-# NOINLINE ni_mapM_ #-}
ni_mapM_ = mapM_

main2 :: IO ()
main2 = forM_ [1..5] $ \_ -> ni_mapM_ print [1 .. N]
#+END_SRC

One would expect that the above programs uses constant space (because
the list ~[1..N]~ is produced lazily).  However, if one compiles the
above program with full laziness and runs it, one observes a memory
residency proportional to ~N~. This happens because GHC shares the
intermediate list ~[1..N]~ between runs of ~ni_mapM_ print [1 .. N]~.

Let us now consider an equivalent program, be written using our
proposed extension for linear types. (To transpose the example with
minimal changes we have to redefine several basic types and functions
--- in a practical application this would not happen because we would
actually be using a custom streaming library, as de Vries does).


#+BEGIN_SRC haskell
data [a] where
  [] :: [a]
  (:) :: a ⊸ [a] ⊸ [a]

discard :: Int ⊸ IO ()

ni_mapM_ :: (a ⊸ IO b) → [a] ⊸ IO ()
forM_ :: [a] ⊸ (a ⊸ IO ()) → IO ()

main2 ::1 IO ()
main2 = forM_ [1..5] $ \i -> do
  discard i
  ni_mapM_ print [1 .. N]
#+END_SRC

In the above example, it is incorrect to share the intermediate
list. Indeed, performing full laziness would amount to transform the
program into the following form, which is not well-typed:

#+BEGIN_SRC haskell
main2 ::1 IO ()
main2 =
  let xs ::1 [a]
      xs = [1 .. N]
  in forM_ [1..5] $ \i -> do
       discard i
       ni_mapM_ print xs
#+END_SRC

Indeed, the above definition attempts to use ~xs~ several times, while
it is bound only once.

In our proposed extension, one could still write the following
type-correct program, which introduces explicit sharing:

#+BEGIN_SRC haskell
main2 ::1 IO ()
main2 =
  let xs ::ω [a]
      xs = [1 .. N]
  in forM_ [1..5] $ \i -> do
       discard i
       ni_mapM_ print xs
#+END_SRC

Yet, thanks to linearity annotations, the programmer intentionally
marked the expressions which are not supposed to be shared, in effect
precisely controlling where (not) to apply full-laziness. Moreover,
the user of a library written for streams would never have to worry
about inadvertent sharing, because the types of the library functions
would specify exactly the right behavior.

See https://jyp.github.io/pdf/Organ.pdf for how this may look like.

* Diminishing GC pressure
Because linear values cannot be shared, they should in principle not
be subject to GC. Indeed, the consumer of the value (pattern matching)
may very well perform de-allocation on the spot. Thus linear values
can be stored in a heap outside of GC control. Alone, this strategy
will diminish GC usage, but may increase the total running time of the
program (if only because allocation in the GC heap is so efficient
that it beats manual memory management for short-lived object)
[Wakeling and Runciman have experienced this effect]. Yet, the
tradeoff may be worth the trouble if long-tail in latencies is a
bigger problem than absolute runtime.

There is however an improvement to be had on top of the simple
strategy. Namely, to always fuse composition of linear functions. This
a good strategy because:

1. It potentially removes many short-lived objects.
2. Fusing *always* is safe performance-wise thanks to linearity.
3. It make the behavior of the compiler more predictable. Thus
   ultimately the behavior of the generated code is more predictable
   too. A drawback is that the optimizer may not be able to perform
   this inlining in practice; however, for the case of Church-encoded
   structures presented earlier in this document, the strategy will
   work.



#  LocalWords:  inliner eg Unfused Lippmeier et al SRC functionals ss
#  LocalWords:  copySetP FilePath srcs dsts sourceFs sk sinkFs drainP
#  LocalWords:  affine duplications GC de Wakeling Runciman Mul Vries
#  LocalWords:  tradeoff latencies runtime exampleClient fno ni mapM
#  LocalWords:  exampleServer NOINLINE forM xs Lindley inlining
#  LocalWords:  expensiveComputation
